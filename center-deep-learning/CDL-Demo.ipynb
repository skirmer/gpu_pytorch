{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask Accelerated Deep Learning\n",
    "\n",
    "Stephanie Kirmer  \n",
    "Senior Data Scientist  \n",
    "[Saturn Cloud](saturncloud.io)  \n",
    "\n",
    "www.stephaniekirmer.com  \n",
    "@data_stephanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief Introduction to Dask\n",
    "\n",
    "Dask is an open-source framework that enables parallelization of Python code.\n",
    "\n",
    "Two key concepts:\n",
    "* Distributed data objects\n",
    "* Distributed computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed Data\n",
    "\n",
    "Data is broken up across multiple machines, allowing analysis on data larger than any single machine's memory.\n",
    "\n",
    "![](img/dask_df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed Computation\n",
    "\n",
    "By using \"lazy\" evaluation, tasks can be organized and queued into DAGs/task graphs for distribution to workers and later computation.\n",
    "\n",
    "![](img/dask_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[notes]\n",
    "The foundation that makes this possible is what's called \"lazy\" evaluation or delayed evaluation. By creating delayed-evaluation tasks, you can develop task graphs, and distribute these across your compute resources to be run simultaneously. This may be used on single machines as well as clusters.\n",
    "\n",
    "This example shows an interconnected task graph of several delayed functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dask Clusters\n",
    "\n",
    "When we implement the Dask framework across multiple machines, the cluster architecture looks something like this. In this structure, we can distribute tasks to the various machines, and return results in aggregate to the client.\n",
    "\n",
    "![](img/dask-cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications for Deep Learning\n",
    "\n",
    "* Process extremely large data using distributed data objects and/or lazy loading\n",
    "* Train very large or complex models using distributed training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed Training\n",
    "\n",
    "* Training a single model across multiple machines simultaneously\n",
    "* Break training data into subsets, each worker handles a different chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[notes] By applying these foundations to deep learning tasks, we can expand the computation possible in a single unit of time - this includes training a single model on multiple machines simultaneously, scaling the training speed. \n",
    "\n",
    "In this demonstration, I'll apply the PyTorch Distributed Data Parallel framework to allow training an image classification model across a cluster. This allows the workers to communicate at intervals, sharing learning acquired during the iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/step1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Demonstration\n",
    "\n",
    "Training image classification model\n",
    "\n",
    "* Architecture: Resnet50 (not pretrained)\n",
    "* Dataset: Stanford Dogs (20,580 images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Elements\n",
    "\n",
    "* Lazy, parallelized loading of training images (S3 to DataLoader)\n",
    "* Distributed training across cluster, one job per worker\n",
    "* Use GPU machines for computation\n",
    "* Performance monitoring outside training context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#%run -i run_cluster_pyt.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "demo-ext-wandb-37",
   "language": "python",
   "name": "demo-ext-wandb-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
