{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Image Classifier on GPU Cluster\n",
    "\n",
    "This tutorial uses Saturn Cloud to access a GPU cluster. This is free for up to three hours per month of GPU usage. For more information about setup, visit https://www.saturncloud.io/docs/. \n",
    "\n",
    "Here I'm also using Weights and Biases, a model performance monitoring tool, to demonstrate the training speed and performance. To learn more about using Weights and Biases with a Saturn Cloud cluster, check out the tutorial at https://github.com/saturncloud/weights-and-biases/.\n",
    "\n",
    "* https://www.saturncloud.io/\n",
    "* https://wandb.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific libraries for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from dask_pytorch_ddp import data, dispatch\n",
    "import torch.distributed as dist\n",
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import helper functions and some additional libraries\n",
    "* Label formatting\n",
    "* Data preprocessing\n",
    "* Plotting results\n",
    "\n",
    "\n",
    "```\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data, \n",
    "    sampler=train_sampler, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    multiprocessing_context=mp.get_context('fork')\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i fns.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Set preference for GPU resources and assign model hyperparameters, training data location, and [Saturn Cloud project ID for accessing GPU cluster](https://www.saturncloud.io/docs/getting-started/external_connect/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============== Constants ============== ###\n",
    "# Fill in your preferred values, including your Saturn Cloud project ID\n",
    "model_params = {'n_epochs': 6, \n",
    "    'batch_size': 100,\n",
    "    'base_lr': .01,\n",
    "    'train_pct': .7,\n",
    "    'downsample_to':1,\n",
    "    'subset': True, # Whether to break data into N pieces for training\n",
    "    'worker_ct': 6, # N of pieces to break into\n",
    "    'bucket': \"saturn-public-data\",\n",
    "    'prefix': \"dogs/Images\",\n",
    "    'pretrained_classes':imagenetclasses} \n",
    "\n",
    "wbargs = {**model_params,\n",
    "    'classes':120,\n",
    "    'dataset':\"StanfordDogs\",\n",
    "    'architecture':\"ResNet\"}\n",
    "\n",
    "project_id = 'a2ae799b6f234f09bd0341aa9769971f'\n",
    "num_workers = 40 # For lazy dataloader multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_transfer_learn(bucket, prefix, train_pct, batch_size, downsample_to,\n",
    "                          n_epochs, base_lr, pretrained_classes, subset, worker_ct):\n",
    "\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    \n",
    "    # --------- Format model and params --------- #\n",
    "    device = torch.device(\"cuda\")\n",
    "    net = models.resnet50(pretrained=True) # True means we start with the imagenet version\n",
    "    model = net.to(device)\n",
    "    model = DDP(model)\n",
    "    \n",
    "    # Set up monitoring\n",
    "    if worker_rank == 0:\n",
    "        wandb.init(config=wbargs, reinit=True, project = 'cdl-demo')\n",
    "        wandb.watch(model)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9)\n",
    "    \n",
    "    # --------- Retrieve data for training and eval --------- #\n",
    "    # Creates lazy-loading, multiprocessing DataLoader objects\n",
    "    # for training and evaluation\n",
    "    \n",
    "    whole_dataset = preprocess(bucket, prefix, pretrained_classes)\n",
    "    \n",
    "    train, val = train_test_split(\n",
    "        train_pct,\n",
    "        whole_dataset, \n",
    "        batch_size=batch_size,\n",
    "        downsample_to=downsample_to,\n",
    "        subset = subset, \n",
    "        workers = worker_ct\n",
    "    )\n",
    "    \n",
    "    dataloaders = {'train' : train, 'val': val}\n",
    "\n",
    "    # --------- Start iterations --------- #\n",
    "    for epoch in range(n_epochs):\n",
    "        count = 0\n",
    "        t_count = 0\n",
    "        \n",
    "    # --------- Training section --------- #    \n",
    "        model.train()  # Set model to training mode\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            perct = [torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "                \n",
    "            # Record the results of this model iteration (training sample) for later review.\n",
    "            if worker_rank == 0:\n",
    "                wandb.log({\n",
    "                        'loss': loss.item(),\n",
    "                        'learning_rate':base_lr, \n",
    "                        'correct':correct, \n",
    "                        'epoch': epoch, \n",
    "                        'count': count\n",
    "                    })\n",
    "            if worker_rank == 0 and count % 5 == 0:\n",
    "                wandb.log({f'predictions vs. actuals, training, epoch {epoch}, count {count}': plot_model_performance(\n",
    "                    model, inputs, labels, preds, perct, pretrained_classes)})\n",
    "                \n",
    "    # --------- Evaluation section --------- #   \n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            for inputs_t, labels_t in dataloaders[\"val\"]:\n",
    "                dt = datetime.datetime.now().isoformat()\n",
    "\n",
    "                inputs_t, labels_t = inputs_t.to(device), labels_t.to(device)\n",
    "                outputs_t = model(inputs_t)\n",
    "                _, pred_t = torch.max(outputs_t, 1)\n",
    "                perct_t = [torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(pred_t, outputs_t)]\n",
    "\n",
    "                loss_t = criterion(outputs_t, labels_t)\n",
    "                correct_t = (pred_t == labels_t).sum().item()\n",
    "            \n",
    "                t_count += 1\n",
    "\n",
    "                # Record the results of this model iteration (evaluation sample) for later review.\n",
    "                if worker_rank == 0:\n",
    "                    wandb.log({\n",
    "                        'val_loss': loss_t.item(),\n",
    "                        'val_correct':correct_t, \n",
    "                        'epoch': epoch, \n",
    "                        'count': t_count\n",
    "                    })\n",
    "                if worker_rank == 0 and count % 5 == 0:\n",
    "                    wandb.log({f'predictions vs. actuals, eval, epoch {epoch}, count {t_count}': plot_model_performance(\n",
    "                        model, inputs_t, labels_t, pred_t, perct_t, pretrained_classes)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "If you'll be using Weights and Biases to train, check to make sure your instance is logged in appropriately. [As the instructions show](https://github.com/saturncloud/weights-and-biases/), if you want to monitor the model training on the cluster, the login code needs to be in the Start Script for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturn Connection Setup\n",
    "\n",
    "Load your user token, [as described in the documentation](https://www.saturncloud.io/docs/getting-started/external_connect/), and create the connection to your project that allows cluster construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_saturn.external.ExternalConnection at 0x7f8238de0650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('config.json') as f:\n",
    "    tokens = json.load(f)\n",
    "\n",
    "conn = ExternalConnection(\n",
    "    project_id=project_id,\n",
    "    base_url='https://app.internal.saturnenterprise.io',\n",
    "    saturn_token=tokens['api_token']\n",
    ")\n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start GPU Cluster\n",
    "\n",
    "The free tier of Saturn Cloud service only allows 3 GPU workers in the cluster, but you can use more at the paid level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dask-saturn:Cluster is ready\n",
      "INFO:dask-saturn:Registering default plugins\n",
      "INFO:dask-saturn:{'tcp://192.168.13.3:40541': {'status': 'repeat'}, 'tcp://192.168.206.131:38741': {'status': 'repeat'}, 'tcp://192.168.211.131:45811': {'status': 'repeat'}, 'tcp://192.168.3.195:37211': {'status': 'repeat'}, 'tcp://192.168.47.195:45463': {'status': 'repeat'}, 'tcp://192.168.5.3:42831': {'status': 'repeat'}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tls://d-steph-cdl-demo-fa90a721acb8498caea5f7a29a297b25.internal.saturnenterprise.io:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-steph-cdl-demo-fa90a721acb8498caea5f7a29a297b25.internal.saturnenterprise.io' target='_blank'>https://d-steph-cdl-demo-fa90a721acb8498caea5f7a29a297b25.internal.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>6</li>\n",
       "  <li><b>Cores: </b>96</li>\n",
       "  <li><b>Memory: </b>381.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://192.168.178.131:8786' processes=6 threads=96, memory=381.00 GB>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = SaturnCluster(\n",
    "    external_connection=conn,\n",
    "    n_workers=6,\n",
    "    worker_size='g4dn4xlarge',\n",
    "    scheduler_size='2xlarge',\n",
    "    nthreads=16)\n",
    "\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(6)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Function on Cluster\n",
    "\n",
    "Distribute the training function and arguments to the cluster, where the parallel training process will take place. At this point, the model training and system resource performance can be visualized on Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: pending, key: dispatch_with_ddp-a7b6d76009a97050cfb671a7fb0ca369>,\n",
       " <Future: pending, key: dispatch_with_ddp-deb2bfe07879ea5b9c4988f0f57524d1>,\n",
       " <Future: pending, key: dispatch_with_ddp-dc1b3ba2d9deeebdc1b76ff86e04d027>,\n",
       " <Future: pending, key: dispatch_with_ddp-402f833334be2d0ca2c690926934789c>,\n",
       " <Future: pending, key: dispatch_with_ddp-3afc20d308e8b6d62e61fa3b5f1574ae>,\n",
       " <Future: pending, key: dispatch_with_ddp-ac456693a53226931e33e85e95874720>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futures = dispatch.run(\n",
    "    client, \n",
    "    cluster_transfer_learn, \n",
    "    **model_params\n",
    "    )\n",
    "\n",
    "futures\n",
    "#futures[0].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "### Cluster distributed training with Dask can speed up deep learning training with no loss of performance\n",
    "\n",
    "### Cluster access is easier and more affordable than you might think\n",
    "\n",
    "### Pay attention to data loading speed, as this can be a bottleneck\n",
    "\n",
    "### Ensure that GPUs are being used to full potential, to avoid excess cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-ext-wandb-37",
   "language": "python",
   "name": "demo-ext-wandb-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
