{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Image Classifier on GPU Cluster\n",
    "\n",
    "This tutorial uses Saturn Cloud to access a GPU cluster. This is free for up to three hours per month of GPU usage. For more information about setup, visit [Saturn Cloud documentation](https://www.saturncloud.io/docs/getting-started/external_connect/). \n",
    "\n",
    "Here I'm also using Weights and Biases, a model performance monitoring tool, to demonstrate the training speed and performance. To learn more about using Weights and Biases with a Saturn Cloud cluster, check out [the tutorial](https://github.com/saturncloud/weights-and-biases/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific libraries for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from dask_pytorch_ddp import data, dispatch\n",
    "import torch.distributed as dist\n",
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import helper functions and some additional libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i fns.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Set preference for GPU resources and assign model hyperparameters, training data location, and [Saturn Cloud project ID for accessing GPU cluster](https://www.saturncloud.io/docs/getting-started/external_connect/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============== Constants ============== ###\n",
    "# Fill in your preferred values, including your Saturn Cloud project ID\n",
    "model_params = {'n_epochs': 6, \n",
    "    'batch_size': 100,\n",
    "    'base_lr': .01,\n",
    "    'train_pct': .7,\n",
    "    'downsample_to':1,\n",
    "    'subset': True, # Whether to break data into N pieces for training\n",
    "    'worker_ct': 6, # N of pieces to break into\n",
    "    'bucket': \"saturn-public-data\",\n",
    "    'prefix': \"dogs/Images\",\n",
    "    'pretrained_classes':imagenetclasses} \n",
    "\n",
    "wbargs = {**model_params,\n",
    "    'classes':120,\n",
    "    'dataset':\"StanfordDogs\",\n",
    "    'architecture':\"ResNet\"}\n",
    "\n",
    "project_id = 'a2ae799b6f234f09bd0341aa9769971f'\n",
    "num_workers = 40 # For lazy dataloader multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_transfer_learn(bucket, prefix, train_pct, batch_size, downsample_to,\n",
    "                          n_epochs, base_lr, pretrained_classes, subset, worker_ct):\n",
    "\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    \n",
    "    # --------- Format model and params --------- #\n",
    "    device = torch.device(\"cuda\")\n",
    "    net = models.resnet50(pretrained=False) # True means we start with the imagenet version\n",
    "    model = net.to(device)\n",
    "    model = DDP(model)\n",
    "    \n",
    "    # Set up monitoring\n",
    "    if worker_rank == 0:\n",
    "        wandb.init(config=wbargs, reinit=True, project = 'cdl-demo')\n",
    "        wandb.watch(model)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9)\n",
    "    \n",
    "    # --------- Retrieve data for training and eval --------- #\n",
    "    # Creates lazy-loading, multiprocessing DataLoader objects\n",
    "    # for training and evaluation\n",
    "    \n",
    "    whole_dataset = preprocess(bucket, prefix, pretrained_classes)\n",
    "    \n",
    "    train, val = train_test_split(\n",
    "        train_pct,\n",
    "        whole_dataset, \n",
    "        batch_size=batch_size,\n",
    "        downsample_to=downsample_to,\n",
    "        subset = subset, \n",
    "        workers = worker_ct\n",
    "    )\n",
    "    \n",
    "    dataloaders = {'train' : train, 'val': val}\n",
    "\n",
    "    # --------- Start iterations --------- #\n",
    "    for epoch in range(n_epochs):\n",
    "        count = 0\n",
    "        t_count = 0\n",
    "        \n",
    "    # --------- Training section --------- #    \n",
    "        model.train()  # Set model to training mode\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            perct = [torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "                \n",
    "            # Record the results of this model iteration (training sample) for later review.\n",
    "            if worker_rank == 0:\n",
    "                wandb.log({\n",
    "                        'loss': loss.item(),\n",
    "                        'learning_rate':base_lr, \n",
    "                        'correct':correct, \n",
    "                        'epoch': epoch, \n",
    "                        'count': count\n",
    "                    })\n",
    "            if worker_rank == 0 and count % 5 == 0:\n",
    "                wandb.log({f'predictions vs. actuals, training, epoch {epoch}, count {count}': plot_model_performance(\n",
    "                    model, inputs, labels, preds, perct, pretrained_classes)})\n",
    "                \n",
    "    # --------- Evaluation section --------- #   \n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            for inputs_t, labels_t in dataloaders[\"val\"]:\n",
    "                dt = datetime.datetime.now().isoformat()\n",
    "\n",
    "                inputs_t, labels_t = inputs_t.to(device), labels_t.to(device)\n",
    "                outputs_t = model(inputs_t)\n",
    "                _, pred_t = torch.max(outputs_t, 1)\n",
    "                perct_t = [torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(pred_t, outputs_t)]\n",
    "\n",
    "                loss_t = criterion(outputs_t, labels_t)\n",
    "                correct_t = (pred_t == labels_t).sum().item()\n",
    "            \n",
    "                t_count += 1\n",
    "\n",
    "                # Record the results of this model iteration (evaluation sample) for later review.\n",
    "                if worker_rank == 0:\n",
    "                    wandb.log({\n",
    "                        'val_loss': loss_t.item(),\n",
    "                        'val_correct':correct_t, \n",
    "                        'epoch': epoch, \n",
    "                        'count': t_count\n",
    "                    })\n",
    "                if worker_rank == 0 and count % 5 == 0:\n",
    "                    wandb.log({f'predictions vs. actuals, eval, epoch {epoch}, count {t_count}': plot_model_performance(\n",
    "                        model, inputs_t, labels_t, pred_t, perct_t, pretrained_classes)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "If you'll be using Weights and Biases to train, check to make sure your instance is logged in appropriately. [As the instructions show](https://github.com/saturncloud/weights-and-biases/), if you want to monitor the model training on the cluster, the login code needs to be in the Start Script for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturn Connection Setup\n",
    "\n",
    "Load your user token, [as described in the documentation](https://www.saturncloud.io/docs/getting-started/external_connect/), and create the connection to your project that allows cluster construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    tokens = json.load(f)\n",
    "\n",
    "conn = ExternalConnection(\n",
    "    project_id=project_id,\n",
    "    base_url='https://app.internal.saturnenterprise.io',\n",
    "    saturn_token=tokens['api_token']\n",
    ")\n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start GPU Cluster\n",
    "\n",
    "The free tier of Saturn Cloud service only allows 3 GPU workers in the cluster, but you can use more at the paid level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = SaturnCluster(\n",
    "    external_connection=conn,\n",
    "    n_workers=6,\n",
    "    worker_size='g4dn4xlarge',\n",
    "    scheduler_size='2xlarge',\n",
    "    nthreads=16)\n",
    "\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(6)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Function on Cluster\n",
    "\n",
    "Distribute the training function and arguments to the cluster, where the parallel training process will take place. At this point, the model training and system resource performance can be visualized on Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = dispatch.run(\n",
    "    client, \n",
    "    cluster_transfer_learn, \n",
    "    **model_params\n",
    "    )\n",
    "\n",
    "futures\n",
    "#futures[0].result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-ext-wandb-37",
   "language": "python",
   "name": "demo-ext-wandb-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
