# Scaling Up PyTorch with GPU Cluster Computing

This repo contains materials for my upcoming presentation at [REWORK's Deep Learning Summit](https://www.re-work.co/events/deep-learning-summit) in January 2021. Let me know on twitter [@data_stephanie](https://twitter.com/data_stephanie) if you have questions!

## What to Expect

In this session, attendees will get a short overview of GPU computing and why it is valuable for deep learning tasks, and then will be walked through an example of using GPU cluster computing with parallelization to conduct a high volume image classification task with Resnet50 in Pytorch. We'll discuss the advantages and drawbacks to using GPU clusters, including speed, cost, and ease of use. See above for the slides discussing GPU computing, and the Jupyter notebook/HTML slides show the demo code.

## Handy Links

[Saturn Cloud](https://www.saturncloud.io/s/): my company, where you can get support and tools to run PyTorch and other ML on GPUs with Dask!

### About GPUs
* https://developer.nvidia.com/opencv
* https://github.com/rapidsai/cuml
* https://github.com/rapidsai/cudf
* https://github.com/rapidsai/cugraph
* https://cupy.dev/
* https://numba.pydata.org/

### Nvidia Documentation
* https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf
* https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html

